{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework3, Neural Network\n",
    "\n",
    "In this section, you will implement a non-linear classifier using the Neural Network.\n",
    "\n",
    "Let's start with a simple code using Neural Network.\n",
    "\n",
    "First, get prepared by importing the numpy library as 'np'.(Or just run the code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from Quandl import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning AND, OR by Single Perceptron\n",
    "\n",
    "We will start with an intuitive Neural Network, which has no hidden layer at all.\n",
    "\n",
    "> First we will learn the two basic linear logistic classification problems, 'AND' and 'OR'.\n",
    "\n",
    "<img src=\"img/hw3/AND.jpg\" alt=\"Drawing\" style=\"width: 400px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias = 1\n",
    "\n",
    "X = np.array([\n",
    "        [bias, 0,0],\n",
    "        [bias, 0,1],\n",
    "        [bias, 1,0],\n",
    "        [bias, 1,1]\n",
    "    ])\n",
    "\n",
    "Y_AND = np.array([\n",
    "        [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1]\n",
    "    ])\n",
    "\n",
    "Y_OR = np.array([\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem1. Weights for a single perceptron\n",
    "\n",
    "The given theta for learning 'AND' and 'OR' are provided below.\n",
    "\n",
    "Run the code and provide your system the given weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The given weight is only appliable for sigmoid function.\n",
    "\n",
    "TO DO :\n",
    "\n",
    "Fill in the weights w1, w2, w3 for each theta so that the result gives the right answer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#your code here\n",
    "theta_AND = np.array([\n",
    "        [(-30), 20, 20]\n",
    "    ])\n",
    "\n",
    "theta_OR = np.array([\n",
    "        [(-10), 20, 20]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an activation function, we are going to use both the softmax function and the sigmoid function.\n",
    "\n",
    "> Codes for both implementations are described below.\n",
    "\n",
    "<img src=\"img/hw3/sigmoid.png\" alt=\"Drawing\" style=\"width: 350px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem2. Sigmoid function\n",
    "\n",
    "Fill in the missing part of the sigmoid function with your code.\n",
    "\n",
    "For additional explanation, d=True will activate the differentiation of the sigmoid function,\n",
    "\n",
    "while d=False will return the pure sigmoid function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(g, d=False):\n",
    "        if(d):\n",
    "            gradientSigmoid = sigmoid(g) * (1.0 - sigmoid(g))\n",
    "\n",
    "            return gradientSigmoid\n",
    "        else:\n",
    "            normalSigmoid = 1 / (1 + np.exp(-g))\n",
    "            return normalSigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code below, you can confirm that both 'AND' and 'OR' has been successfully implemented using Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND\n",
      "[[ 0.  0.  0.  1.]]\n",
      "\n",
      "OR\n",
      "[[ 0.  1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "flag = False # gradients are only needed in the case for back propagation.\n",
    "\n",
    "\"\"\"\n",
    "Check if AND = [0, 0, 0, 1], OR = [0, 1, 1, 1]\n",
    "\"\"\"\n",
    "\n",
    "h_AND_sigmoid = np.round(sigmoid(np.dot(X, theta_AND.T), flag))\n",
    "print \"AND\"\n",
    "print h_AND_sigmoid.T\n",
    "\n",
    "print \"\"\n",
    "\n",
    "h_OR_sigmoid = np.round(sigmoid(np.dot(X, theta_OR.T), flag))\n",
    "print \"OR\"\n",
    "print h_OR_sigmoid.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning XOR by Neural Network\n",
    "\n",
    "\n",
    "Now, let's move on to the non-linear classifying problems.\n",
    "\n",
    "Until now, 'AND' and 'OR' where both linear classifications, so we could implemented\n",
    "\n",
    "both methods by forward propagation by easily combinating theta's without a hidden layer.\n",
    "\n",
    "However, in the case of non-linear functions such as 'XOR', it's not quite easy to come up with an appropriate theta.\n",
    "\n",
    "From now on, we will adopt another layer(which is called the hidden layer) in order to successfully classify this problem.\n",
    "\n",
    "> Our target is that to return 0 if the two inputs are the same, and 1 if the two are differrent.\n",
    "\n",
    "> Fill in the indicated spaces with your own code for the function 'sigmoid' and 'forward_propagation'.\n",
    "\n",
    "> The 'backward_propagation' learning code part is already implemented.\n",
    "\n",
    "> If your algorithm is correct, the code below will run without any trouble and successfully learn the results  of 'XOR'.\n",
    "\n",
    "<img src=\"img/hw3/XOR.jpg\" alt=\"Drawing\" style=\"width: 400px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem3. Forward Propagation and Cost Function for Neural Network.\n",
    "\n",
    "Neural Network is consisted with 3 important features.\n",
    "\n",
    "> 1. Forward Propagation for prediction.\n",
    "\n",
    "> 2. Backward Propagation for Learning.\n",
    "\n",
    "> 3. Cost Function for providing a threshold to learning.\n",
    "\n",
    "As implementing Backward Propagation by your own will be very tricky, I have wrote the code down for you.\n",
    "\n",
    "You will need to fill in the rest of the function consisting the class.\n",
    "\n",
    "If all your implementations are correct, you might see that the XOR learning below will operate properly.\n",
    "\n",
    "This is a \"Hello World!\" example in the field of Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This will create a Neural Network with specified inputs and outputs.\n",
    "\n",
    "Input : When building a constructor, you should pass on the size of the input layer, the output layer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, InputLayer, OutputLayer):\n",
    "        # Define the number of nodes in each layers\n",
    "        self.inputLayer = InputLayer\n",
    "        self.hiddenLayer = InputLayer\n",
    "        self.output = OutputLayer\n",
    "\n",
    "        # create node weight matrices\n",
    "        self.theta0 = 2*np.random.random((self.hiddenLayer, self.inputLayer+1))-1\n",
    "        self.theta1 = 2*np.random.random((self.output, self.hiddenLayer+1))-1\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, g, d=False):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "        if(d):\n",
    "            gradientSigmoid = sigmoid(g) * (1.0 - sigmoid(g))\n",
    "            return gradientSigmoid\n",
    "        else:\n",
    "            normalSigmoid = (1 / (1 + np.exp(-g)))\n",
    "            return normalSigmoid\n",
    "\n",
    "        \n",
    "    def costFunction(self, X, y):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "        m = X.shape[0] # 'm' represents the number of training examples.\n",
    "        cost = 0\n",
    "        \n",
    "        #for i in range(0,m):\n",
    "        #    cost = cost + y[i]*np.log(self.forward_propagation(X[i])) + (1-y[i])*np.log(1-(self.forward_propagation(X[i])))\n",
    "        h_X = self.forward_propagation(X)\n",
    "        cost = (-1/m)*np.sum(np.multiply(y, np.log(h_X)) + np.multiply((1-y), np.log(1-h_X)))\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "\n",
    "        # Forward Propagate Inputs Through Network\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        bias = np.append(1,X[0])\n",
    "        for j in range(1,m):\n",
    "            biasX = np.append(1,X[j])\n",
    "            bias = np.vstack([bias,biasX])\n",
    "        # Before starting your propagation, you need to add bias in your input unit\n",
    "        self.a1 = bias\n",
    "\n",
    "        # your code here. Add bias nodes to the 2 dimensional array representing your input.\n",
    "\n",
    "        # Input Layer -> Hidden Layer\n",
    "        self.z2 = self.a1.dot(self.theta0.T) # your code here. The dot product of a1 and theta0\n",
    "        self.a2 = sigmoid(self.z2,False) # your code here. The sigmoid activated result of z2\n",
    "\n",
    "        # Before starting next propagation, add bias 1 in your hidden layer unit\n",
    "        bias = np.append(1,self.a2[0])\n",
    "        for i in range(1,self.a2.shape[0]):\n",
    "            biasX = np.append(1,self.a2[i])\n",
    "            bias = np.vstack([bias,biasX])\n",
    "        self.a2 = bias\n",
    "\n",
    "\n",
    "        # Hidden Layer -> Output Layer\n",
    "        self.z3 = self.a2.dot(self.theta1.T)# your code here. The dot product of a2 and theta1\n",
    "        self.ho = sigmoid(self.z3,False)# your code here. The sigmoid activated result of z3\n",
    "        \n",
    "        # Final output of the neural network\n",
    "        return self.ho\n",
    "\n",
    "    def backward_propagation(self, X, y, alpha):\n",
    "        # for e in range(epoch):\n",
    "        max_iteration = 500\n",
    "        self.err = np.ones(max_iteration)\n",
    "        final_error = 1\n",
    "        iteration = 0\n",
    "        \n",
    "        while self.err[iteration] > 1e-10 :\n",
    "            # Forward propogate\n",
    "            a3 = self.forward_propagation(X)\n",
    "            \n",
    "            # Backward Propagate\n",
    "            delta3 = a3 - y\n",
    "            theta1_grad = np.dot(delta3.T, self.a2)\n",
    "            delta2 = np.dot(delta3, self.theta1[:, 1:]) * self.sigmoid(self.z2, d=True)\n",
    "            theta0_grad = np.dot(delta2.T, self.a1)\n",
    "\n",
    "            change = theta0_grad\n",
    "            self.theta0 -= (alpha-change*0.1) * theta0_grad\n",
    "            change = theta1_grad\n",
    "            self.theta1 -= (alpha-change*0.1) * theta1_grad\n",
    "            \n",
    "            if(iteration + 1 == 500):\n",
    "                return self.err\n",
    "            \n",
    "            final_error = self.costFunction(X, y)\n",
    "            self.err[iteration+1] = final_error\n",
    "            iteration = iteration+1\n",
    "            \n",
    "        return self.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beneath is the training example for XOR code. The results should be an hypothesis array of [0, 1, 1, 0]\n",
    "\n",
    "<img src=\"img/hw3/XOR_network.jpg\" alt=\"Drawing\" style=\"width: 200px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XOR training data\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your own sake, we also implemented a plotting system so you could visually observe the error eventually converging to 0.\n",
    "\n",
    "We also implemented a variety of skills in order to prevent the gradient descent from falling into a local minimum.\n",
    "\n",
    "Check out the code for further notations.\n",
    "\n",
    "> You should close the pop-up screen(showing you the cost graph, visualizing your error respect to iterations)\n",
    "\n",
    "> in order to observe your final result.\n",
    "\n",
    "> The final result should be a numpy array of [0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error before training =  2.94647407224\n",
      "Error after training =  0.0532731877417\n",
      "[[ 0.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2, 1)\n",
    "\n",
    "print \"Error before training = \", nn.costFunction(X, y)\n",
    "\n",
    "# learning thetas\n",
    "max_iteration = 500\n",
    "min_err = 1\n",
    "for i in range(10):\n",
    "    nn = NeuralNetwork(2, 1)\n",
    "    cost = np.zeros(max_iteration)\n",
    "    cost = nn.backward_propagation(X, y, 0.5)         # learning rate = 0.5 , epoch = 1\n",
    "    err = nn.costFunction(X, y)\n",
    "    \n",
    "    if (min_err > err):\n",
    "        min_err = err\n",
    "        result = nn.forward_propagation(X)\n",
    "        min_cost = cost\n",
    "\n",
    "print \"Error after training = \", min_err\n",
    "print np.round(result.T)\n",
    "# visualize cost: y-axis, iterations(epochs): x-axis\n",
    "plt.plot(np.arange(0., max_iteration, 1.0), min_cost.T)\n",
    "plt.axis([0, max_iteration, 0, 1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network in Finance\n",
    "\n",
    "Now we are going to move on to the application of Neural Network, especially in the field of finance.\n",
    "\n",
    "We are going to take 3 simple features in finance(S&P500, Dow Jones IA, PIR) and will predict the NASDAQ index.\n",
    "\n",
    "We will crawl the data we need from Yahoo finance.\n",
    "\n",
    "But, at first, beneath is a brief explanation of the indices above for those you are not familiar with finance.\n",
    "\n",
    "> Reading the material below is purely optional, so you may as well skip and move on to the coding part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pattern import web\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. S&P500\n",
    "\n",
    "The S&P500 is a free-float capitalization-weighted index published since 1957 of the prices of 500 large-cap common stocks actively traded in the United States. The stocks included in the S&P500 are those of large publicly held companies that trade on either of the two largest American stock market companies; the NYSE Euronext and the NASDAQ OMX. Actually, the S&P500 is one of the most widely followed indexes of large-cap American stocks. It is considered a bellwether for the American economy, and is included in the Index of Leading Indicators. S&P500 index fluctuations are dependent upon a lot of factors, thus the entire prediction pattern is very complex. In this application, the input data is represented only by historical items of 4 important economical indicators. It is essential to mention that if you want a better predictor, you should feed your neural network with more indicators that are more or less important for the entire interpolation. \n",
    "\n",
    "<img src=\"img/hw3/SP500.png\" alt=\"Drawing\" style=\"width: 500px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem4. Crawling Information from Yahoo Finance.\n",
    "\n",
    "As you have previously done in Homework1, you have to scrap the information from Yahoo finance.\n",
    "\n",
    "Those who have done Homework1 on there own will have no trouble in following the rest of the implementation.\n",
    "\n",
    "Questions about specific methods for Crawling information will NOT be guided.\n",
    "\n",
    "<img src=\"img/hw3/Screenshot.png\" alt=\"Drawing\" style=\"float: left;\"/>\n",
    "\n",
    "Above is a screenshot for the Yahoo Finance S&P500 page.\n",
    "\n",
    "> Fill in the function 'scrap_page' below in order to crawl the given URL and retreive the information\n",
    "\n",
    "(Monthly Historical prices for Adjacent Close).\n",
    "\n",
    "The URLs you need are provided below.\n",
    "\n",
    "> S&P500 URL = http://finance.yahoo.com/q/hp?s=%5EGSPC&g=m\n",
    "\n",
    "> Dow Jones Industrial Average URL = http://finance.yahoo.com/q/hp?s=%5EDJI&g=m\n",
    "\n",
    "> NASDAQ Composite URL = http://finance.yahoo.com/q/hp?s=NDAQ&g=m\n",
    "\n",
    "> PIR Composite URL = http://finance.yahoo.com/q/hp?s=PIR&g=m\n",
    "\n",
    "You will notice that the URL is consisted with 'http://finance.yahoo.com/q/hp?s=' + id + '&g=m'\n",
    "\n",
    "Utilize this in order to fill in the 'scrap_page' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_page(request_id):\n",
    "    print \"fetching \" , request_id\n",
    "        \n",
    "    target_url = \"http://finance.yahoo.com/q/hp?s=\" + request_id + \"&g=m\"\n",
    "    r = requests.get(target_url)\n",
    "    \n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching  %5EGSPC\n"
     ]
    }
   ],
   "source": [
    "SP500_code = '%5EGSPC'\n",
    "SP500_html = scrap_page(SP500_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beneath is a str2number function you have already implemented in your previous Homework1.\n",
    "\n",
    "You DON'T need to do anything to the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2number(number_str):\n",
    "        if number_str is None:\n",
    "            return None\n",
    "        else:\n",
    "            number_str = re.sub(\"[^0-9]\", \"\", number_str) \n",
    "            # return int(number_str)\n",
    "            if(number_str == \"\") : \n",
    "                number_str = '1';\n",
    "            return int(float(number_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem5. Parsing HTML\n",
    "\n",
    "Parse the html code retrieved in order to get only the informations we need.\n",
    "\n",
    "We only need the information for the Adj. Close for each month.\n",
    "\n",
    "Ignore the other data.\n",
    "\n",
    "Write your code so the function beneath plays the exact roll I described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsing(html):\n",
    "    element = web.Element(html)\n",
    "    \n",
    "    ele = (element('table.yfnc_datamodoutline1 table tr'))\n",
    "    \n",
    "    \n",
    "    # tr 을 지워서 테이블 개수를 확인. 만약에 두 개 이상이면 뒤에 배열로 접근.\n",
    "    index = len(ele)\n",
    "    lst = []\n",
    "    # Check what is being retrieved in your system constantly.\n",
    "    for i in range(0,index):\n",
    "        ele2 = web.Element(ele[i].content)\n",
    "        ele3 = ele2('td')\n",
    "        if (len(ele3) > 5):\n",
    "            lst.append(str2number(ele3[6].content))\n",
    "    return lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the result list should be 67 as it is represented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP500 = parsing(SP500_html)\n",
    "len(SP500) # This should be 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the 3 indices below, we will do the same operations as we did to the 'S&P500' index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dow Jones Industrial Average\n",
    "\n",
    "The Dow Jones Industrial Average (DJIA), also referred to as the Industrial Average, the Dow Jones, the Dow 30, or simply the Dow, is a stock market index, and one of several indices created by Wall Street Journal editor and Dow Jones & Company co-founder Charles Dow. It is an index that shows how 30 large, publicly owned companies based in the United States have traded during a standard trading session in the stock market. Along with the NASDAQ Composite, the S&P500 Index, and the Russell 2000 Index, the Dow is among the most closely watched benchmark indices tracking targeted stock market activity. To calculate the DJIA, the sum of the prices of all 30 stocks is divided by a Divisor, the Dow Divisor. The divisor is adjusted in case of stock splits, spinoffs or similar structural changes, to ensure that such events do not in themselves alter the numerical value of the DJIA.\n",
    "\n",
    "<img src=\"img/hw3/DowJones.jpg\" alt=\"Drawing\" style=\"width: 500px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching  %5EDJI\n"
     ]
    }
   ],
   "source": [
    "DowJones_code = '%5EDJI'\n",
    "DowJones_html = scrap_page(DowJones_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DJI = parsing(DowJones_html)\n",
    "len(DJI) # This should be 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prime Interest Rate\n",
    "\n",
    "Prime rate, or Prime Lending Rate, is a term applied in many countries to a reference interest rate used by banks. The term originally indicated the rate of interest at which banks lent to favored customers, i.e., those with high credibility, though this is no longer always the case. Some variable interest rates may be expressed as a percentage above or below prime rate. Generally, prime interest rate is a significant determinant in the world of financial marketing. This is because monetary policy is aimed at influencing domestic interest rates, which drive currency rates relative to other currencies with different interest rates. Domestic interest rates also influence overall economic activity, with lower interest rates typically stimulating borrowing, investment, and consumption, while higher interest rates tend to reduce borrowing, and increase saving over consumption. Below is shown Federal Funds Rate History graph. This data will be used in the current application.\n",
    "\n",
    "<img src=\"img/hw3/PIR.png\" alt=\"Drawing\" style=\"width: 500px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching  PIR\n"
     ]
    }
   ],
   "source": [
    "PIR_code = 'PIR'\n",
    "PIR_html = scrap_page(PIR_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIR = parsing(PIR_html)\n",
    "len(PIR) # This should be 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NASDAQ Composite\n",
    "\n",
    "The NASDAQ Composite is a stock market index of the common stocks and similar securities listed on the NASDAQ stock market, meaning that it has over 3,000 components. It is highly followed in the U.S. as an indicator of the performance of stocks of technology companies and growth companies. Since both U.S. and non-U.S. companies are listed on the NASDAQ stock market, the index is not exclusively a U.S. index.\n",
    "\n",
    "<img src=\"img/hw3/NASDAQ.png\" alt=\"Drawing\" style=\"width: 500px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching  NDAQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NSDQ_code = 'NDAQ'\n",
    "NSDQ_html = scrap_page(NSDQ_code)\n",
    "NSDQ = parsing(NSDQ_html)\n",
    "len(NSDQ) # This should be 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting it together\n",
    "\n",
    "### Normalization\n",
    "\n",
    "This code is optional. Try it out after you have finished your code and checked your original results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SP500 = (SP500 - np.mean(SP500))/np.std(SP500)\n",
    "# DJI = (DJI-np.mean(DJI))/np.std(DJI)\n",
    "# PIR = (PIR-np.mean(PIR))/np.std(PIR)\n",
    "# NSDQ = (NSDQ-np.mean(NSDQ))/np.std(NSDQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem6. Constructing an Input Matrix\n",
    "\n",
    "First, in order to shuffle your data set to prevent it from getting biased by time, you will need to construct a one large\n",
    "\n",
    "matrix containing all the informations you have previously crawled from 'Yahoo Finance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mtrx = np.append(SP500[0],DJI[0])\n",
    "mtrx = np.append(mtrx,PIR[0])\n",
    "mtrx = np.append(mtrx,NSDQ[0])\n",
    "for i in range (1,len(PIR)):\n",
    "    mtrx2 = np.append(SP500[i],DJI[i])\n",
    "    mtrx2 = np.append(mtrx2,PIR[i])\n",
    "    mtrx2 = np.append(mtrx2,NSDQ[i])\n",
    "    mtrx = np.vstack([mtrx,mtrx2])\n",
    "# your code here. Matrix for every indices you have retrieved. It should have the size of (67 x 4)\n",
    "# Notice that the last column will be seperated as your target value 'y'.\n",
    "\n",
    "# So make sure that the 'NASDAQ' index comes in the last column of your matrix.\n",
    "# For example, [[S&P500 column], [PIR column], [Dow Jones column], [NASDAQ column]] will be a valid matrix\n",
    "np.random.shuffle(mtrx) # Shuffle the matrix using 'shuffle' function in 'numpy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem7. Constructing Data sets\n",
    "\n",
    "\n",
    "#### (1) X, y\n",
    " Now on, you will construct data sets X and Y.\n",
    "\n",
    "DO NOT split your data into training and cross validation sets yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = mtrx[:,:3]\n",
    "y = mtrx[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-1) train_X\n",
    "\n",
    "Now, spilt your training data, index starting from 13 to the last index 67.\n",
    "\n",
    "The total dimension of the final train_X will be (55L, 3L)\n",
    "\n",
    "Do NOT use the function provied from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_X = X[12:67]\n",
    "train_X.shape # this should be (55L, 3L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### (2-2) train_Y\n",
    "\n",
    "### Once again, split your Y training labels, index starting from 13 to the last index 67.\n",
    "\n",
    "### The total dimension of the final train_Y will be (55L, 1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y = y[12:67]\n",
    "train_Y.shape # this should be (55L, 1L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3-1) test_X\n",
    "\n",
    "Now, take the rest and make it as your test set.\n",
    "\n",
    "This set will be only used for evaluation, and will NOT be used in the process of training.\n",
    "\n",
    "The final dimension of test_X will be (12L, 3L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = X[0:12]\n",
    "test_X.shape # this should be (12L, 3L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3-2) test_Y\n",
    "\n",
    "Do the same for 'y'.\n",
    "\n",
    "The final dimension for test_Y will be (12L, 1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = y[0:12]\n",
    "test_Y.shape # this should be (12L, 1L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem8. Using LASSO to predict the NASDAQ index\n",
    "\n",
    "In this section, you will predict the PIR using the other 3 indices, and get the accuracy of the prediction.\n",
    "\n",
    "We will use the 'Lasso Function' to fit the given model.\n",
    "\n",
    "In order to use Lasso, use 'var = Lasso(input)'\n",
    "\n",
    "A following example is given below.  Fill in the variables with the appropriate methods.\n",
    "\n",
    "Remember, the test set SHOULD NOT BE INCLUDED IN THE TRAINING PROCESS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "clf = Lasso(alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahroobe/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2679.098881293855,\n",
       " 5016.5501720514285,\n",
       " 5297.393708218479,\n",
       " 2426.0180493479193,\n",
       " 5766.792956328559,\n",
       " 2763.1009560205666,\n",
       " 2273.622099974511,\n",
       " 2408.1729703456776,\n",
       " 2717.910065560269,\n",
       " 2578.626062533617,\n",
       " 2584.790274784751,\n",
       " 4943.622213888638]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Lasso(alpha = 0.1)\n",
    "clf.fit(train_X, train_Y)\n",
    "pred = clf.predict(test_X)\n",
    "pred = pred.tolist()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2689, 5003, 6305, 2131, 5795, 2861, 2231, 2435, 2813, 2301, 2263,\n",
       "       4222])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y # Compare with the prediction you did just above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem9. Using Ridge to predict the NASDAQ index\n",
    "\n",
    "In this section, we will do the same steps above, only changing that\n",
    "\n",
    "we will use the 'pipeline model. I already have written down the code \n",
    "\n",
    "In order to use Ridge, use 'var = Lasso(input)'\n",
    "\n",
    "A following example is given below. Fill in the variables with the appropriate methods.\n",
    "\n",
    "Remember, the test set SHOULD NOT BE INCLUDED IN THE TRAINING PROCESS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2153.60009741  4648.58859633  6198.103559    2125.4918388   6069.77157377\n",
      "  3133.4749422   1564.34799682  2043.88960134  2894.8541026   2151.30904441\n",
      "  2327.5152741   4412.64362668  2073.71769475  5730.99976395]\n",
      "[2689 5003 6305 2131 5795 2861 2231 2435 2813 2301 2263 4222 2201 5741]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "min = 1\n",
    "#67 -> 12:55.  test:cv:train -> 14:14:39\n",
    "test_X = X[0:14]\n",
    "test_Y = y[0:14]\n",
    "cv_X = X[14:28]\n",
    "cv_Y = y[14:28]\n",
    "train_X = X[28:67]\n",
    "train_Y = y[28:67]\n",
    "for degree in range(5):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    model.fit(train_X, train_Y)\n",
    "    if(model.score(cv_X, cv_Y) < min):\n",
    "        fin_model = model\n",
    "        minScore = model.score(cv_X,cv_Y)\n",
    "        \n",
    "Xf = fin_model.predict(test_X)\n",
    "print Xf # Compare the result from the test_Y above.\n",
    "print test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9604159604416177"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_model.score(test_X, test_Y) # Observe the results of the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework3 , Support Vector Machine \n",
    "In this section, we will use new dataset,\n",
    "\n",
    "__blog corpus__, which was published at 2006.\n",
    "\n",
    "First of all, download the corpus,\n",
    "\n",
    "from this site → (http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm)\n",
    "\n",
    "And then, unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from glob import glob \n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# set your path to the unzipped blog folder.\n",
    "DATA_FILE_PATH_PATTERN = \"data/blogs/*.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods\n",
    "\n",
    "We will use new package, __\"nltk\"__.\n",
    "\n",
    "Using the package, we can process natural language easily.\n",
    "\n",
    "First of all, we must download some _dictionaries_ to use the package's functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ahroobe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahroobe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionaries, to be used in spliting the words and removing the stop words like \"a, the, ...\"\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we will get the stopwords,\n",
    "\n",
    "and we will make new object, which is from stemmer class.\n",
    "\n",
    "which makes redundancy between two different form of words lower,\n",
    "\n",
    "like beauty and beautiful being merged into \"beaut\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop word set and the stemmer object,\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem10.\n",
    "\n",
    "Now, we need to load files.\n",
    "\n",
    "Each file names are formatted like \"_user_ _ _id_._gender_._age_._fields_._constellation_.xml\".\n",
    "\n",
    "Fill each blanks, and make expected format shown below.\n",
    "\n",
    "(For the age field, we will mark teenager as 1, otherwise 0)\n",
    "\n",
    "__Notice.__ _Because the data is not valid XML format (e.g. include '&' character), don't use XML parser._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_user_info(file_path):\n",
    "\n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    From the file_path,\n",
    "    \n",
    "    extract each records,\n",
    "    \n",
    "    and return dictionary,\n",
    "    \n",
    "    which contains each records,\n",
    "    \n",
    "    user_id, gender, age, fields, constellation\n",
    "    \n",
    "    (WARN: \"age\" column should be labelled with 1, 0,\n",
    "    \n",
    "    which represents the user is teenager or not.)\n",
    "    \"\"\"\n",
    "    rec = dict()\n",
    "    data = file_path.split('.')\n",
    "    rec['user_id']=data[0][-7:]\n",
    "    rec['gender']=data[1]\n",
    "    age = int(data[2])\n",
    "    if (age/10==1):\n",
    "        rec['age']=1\n",
    "    else:\n",
    "        rec['age']=0\n",
    "    rec['fields']=data[3]\n",
    "    rec['constellation']=data[4]\n",
    "\n",
    "    return rec\n",
    "    \n",
    "    ####################################################\n",
    "    \n",
    "    return file_info_dict\n",
    "        \n",
    "def parse_file(file_path):\n",
    "\n",
    "    # parse user_info from file_path string\n",
    "    user_info = parse_user_info(file_path)\n",
    "    \n",
    "    f = open(file_path, \"r\")\n",
    "    tmp_str = \"\"\n",
    "    for l in f:\n",
    "        tmp_str += l.strip()\n",
    "    f.close()\n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    Get only post section from \"tmp_str\"\n",
    "    \n",
    "    Hint: use re package to parse the string.\n",
    "    \n",
    "    wr_tmp_str contains all of the post data.\n",
    "    \"\"\"\n",
    "    element = web.Element(tmp_str)\n",
    "    element = element('post')\n",
    "    wr_tmp_str=\"\"\n",
    "    for ele in element:\n",
    "        wr_tmp_str = wr_tmp_str + ele.content\n",
    "    ####################################################\n",
    "    \n",
    "    # merge post text\n",
    "    user_info['post'] = wr_tmp_str.encode('utf-8', errors = \"replace\")  #-> \" error generated. \n",
    "    #user_info['post'] = wr_tmp_str\n",
    "    return user_info\n",
    "\n",
    "def load_data(file_path_pattern, ratio=1.):\n",
    "\n",
    "    file_paths = glob(file_path_pattern)\n",
    "    file_cnt = int(len(file_paths) * ratio)\n",
    "    \n",
    "    data = list()\n",
    "    cnt = 0\n",
    "    \n",
    "    for file_path in file_paths[:file_cnt]:\n",
    "        user_data = parse_file(file_path)\n",
    "        data.append(user_data)\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            print cnt,'/',file_cnt\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>constellation</th>\n",
       "      <th>fields</th>\n",
       "      <th>gender</th>\n",
       "      <th>post</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Leo</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>Well, everyone got up and going this morning. ...</td>\n",
       "      <td>1000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Libra</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Yeah, sorry for not writing for a whole there,...</td>\n",
       "      <td>1000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>Arts</td>\n",
       "      <td>male</td>\n",
       "      <td>cupid,please hear my cry, cupid, please let yo...</td>\n",
       "      <td>1004904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>Arts</td>\n",
       "      <td>female</td>\n",
       "      <td>and did i mention that i no longer have to dea...</td>\n",
       "      <td>1005076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>male</td>\n",
       "      <td>B-Logs: The Business Blogs Paradox    urlLink ...</td>\n",
       "      <td>1005545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>Religion</td>\n",
       "      <td>male</td>\n",
       "      <td>1/03 DrKioni.com Awarded ByRegion.net Healers ...</td>\n",
       "      <td>1007188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Architecture</td>\n",
       "      <td>female</td>\n",
       "      <td>Friday    My dear wife was walking on her grad...</td>\n",
       "      <td>/100812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Sorry, but I gotta..I couldn't remember the wo...</td>\n",
       "      <td>1008329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>Planning the Marathon   I checked Active.com, ...</td>\n",
       "      <td>1009572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>Technology</td>\n",
       "      <td>female</td>\n",
       "      <td>The astute among you will note that this run i...</td>\n",
       "      <td>1011153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>MSN conversation: 11.17am   Iggbalbollywall  (...</td>\n",
       "      <td>1011289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>Hey!!! Tonight kids, I saw Harry Potter and th...</td>\n",
       "      <td>1011311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>RealEstate</td>\n",
       "      <td>male</td>\n",
       "      <td>Hey you clowns and kids with Down's. Sorry I h...</td>\n",
       "      <td>1013637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>aint no such things as halfway crooks.   this ...</td>\n",
       "      <td>1015252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>Technology</td>\n",
       "      <td>male</td>\n",
       "      <td>well, the retreat was a success, i think. at l...</td>\n",
       "      <td>1015556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>male</td>\n",
       "      <td>In his  urlLink February 14, 2003 post  Ray Ma...</td>\n",
       "      <td>1016560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>male</td>\n",
       "      <td>as astute followers of the  urlLink Assistant ...</td>\n",
       "      <td>1016738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>Leo</td>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>female</td>\n",
       "      <td>You love me... I have you here by my side... O...</td>\n",
       "      <td>1016787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>RealEstate</td>\n",
       "      <td>female</td>\n",
       "      <td>Dear Susan,     You are a fat Wombat... I hate...</td>\n",
       "      <td>1019224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>yay! it changed! :) i think i get it now. you ...</td>\n",
       "      <td>1019622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>Yes i survived not eating for 24 hours, I am g...</td>\n",
       "      <td>1019710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>IN THE SPACESHIP, THE SILVER SPACESHIP�  I fou...</td>\n",
       "      <td>1021779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>Aaaaaahhhh....... mornings! know that it is st...</td>\n",
       "      <td>1022037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>All right, I officially live in THE nut house....</td>\n",
       "      <td>1022086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>Libra</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>wait...now dat i think about it...dat GBC thin...</td>\n",
       "      <td>1024234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Lily's Lullaby- Part SIX  It was cold that eve...</td>\n",
       "      <td>1025783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Education</td>\n",
       "      <td>female</td>\n",
       "      <td>I've posted a new test on my AIM profile...you...</td>\n",
       "      <td>1026164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>Hello hello hello again. It's lovely to speak ...</td>\n",
       "      <td>1026443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Libra</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>OK here goes nothing, I'm starting over. My ol...</td>\n",
       "      <td>1028027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Education</td>\n",
       "      <td>male</td>\n",
       "      <td>Wednesday of this week, my wife Elizabeth and ...</td>\n",
       "      <td>1028257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>0</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>Slept at 10 last night. That's something I hav...</td>\n",
       "      <td>1757463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>Technology</td>\n",
       "      <td>male</td>\n",
       "      <td>week of august 24 - 30, 2003    urlLink Death ...</td>\n",
       "      <td>1758069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>0</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>female</td>\n",
       "      <td>today i feel so cute ! don´t know why, maybe b...</td>\n",
       "      <td>1758432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>female</td>\n",
       "      <td>hi. i'm back. another week past. so, all the e...</td>\n",
       "      <td>1759323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>0</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>Technology</td>\n",
       "      <td>male</td>\n",
       "      <td>I started my new workout regime today.  This r...</td>\n",
       "      <td>1759489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>Law</td>\n",
       "      <td>female</td>\n",
       "      <td>I'm 40 years old, female, single, working as a...</td>\n",
       "      <td>1760992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>0</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>Days off always make me want to waste time wit...</td>\n",
       "      <td>1761003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>1</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>This is how i exactly feel...  Scorpio October...</td>\n",
       "      <td>1761133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>0</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>It works.. I love doing this. :DI found a nift...</td>\n",
       "      <td>1761174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>1</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>hiz... juz wasting time and testing some stuff...</td>\n",
       "      <td>1762421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>0</td>\n",
       "      <td>Aries</td>\n",
       "      <td>Internet</td>\n",
       "      <td>male</td>\n",
       "      <td>This blog is about www.monto-dash.com and its ...</td>\n",
       "      <td>1763114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>1</td>\n",
       "      <td>Virgo</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>[Listening to: Miss You -  urlLink Aaliyah  - ...</td>\n",
       "      <td>1764527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>1</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>HumanResources</td>\n",
       "      <td>male</td>\n",
       "      <td>i wouldn't believe too much about what you hea...</td>\n",
       "      <td>1764573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>male</td>\n",
       "      <td>urlLink movabletype.org  produced a great tool...</td>\n",
       "      <td>1764916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>so, apparently, the mountain decided to come t...</td>\n",
       "      <td>1767662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>1</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>Hey yaw! i kno i havent written in a while so ...</td>\n",
       "      <td>1769548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>0</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>Arts</td>\n",
       "      <td>male</td>\n",
       "      <td>So, I had two topics I was considering for tod...</td>\n",
       "      <td>1769576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>0</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>Agriculture</td>\n",
       "      <td>female</td>\n",
       "      <td>urlLink f**kin' boogers.GAMEDAY!!!   Just got ...</td>\n",
       "      <td>1770781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>1</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>Wow, my first post in a long line of posts to ...</td>\n",
       "      <td>1770846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>1</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>\"going under\"  The rise of exams stress certai...</td>\n",
       "      <td>1771307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>1</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>hey ppl..im in victoria now, its so damn hot!!...</td>\n",
       "      <td>1772041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>1</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>male</td>\n",
       "      <td>please do feel free to leave comments if you w...</td>\n",
       "      <td>1772134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>0</td>\n",
       "      <td>Pisces</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>female</td>\n",
       "      <td>i dun like to wait. i dun like to sit n sulk. ...</td>\n",
       "      <td>1772503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>1</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>hey guys, its been a long time. I dont hav a r...</td>\n",
       "      <td>1774151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>1</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>steven sent me this thing so i decided to fill...</td>\n",
       "      <td>1774842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>0</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>female</td>\n",
       "      <td>so i move to california a month ago. i am incr...</td>\n",
       "      <td>1776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>0</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbs...</td>\n",
       "      <td>1777155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "      <td>without the jukebox. The jukebox that Elaine s...</td>\n",
       "      <td>1778579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>0</td>\n",
       "      <td>Libra</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>female</td>\n",
       "      <td>We got back from the conference to bunch of in...</td>\n",
       "      <td>1778822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>1</td>\n",
       "      <td>Leo</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>Aint I lousy with mine updates.  I'm pretty ex...</td>\n",
       "      <td>1779446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>966 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age constellation                fields  gender  \\\n",
       "0      0           Leo                indUnk  female   \n",
       "1      1         Libra               Student  female   \n",
       "2      0     Capricorn                  Arts    male   \n",
       "3      0        Cancer                  Arts  female   \n",
       "4      0   Sagittarius           Engineering    male   \n",
       "5      0         Libra              Religion    male   \n",
       "6      0         Aries          Architecture  female   \n",
       "7      1        Pisces               Student  female   \n",
       "8      0        Cancer                indUnk    male   \n",
       "9      0         Virgo            Technology  female   \n",
       "10     0         Libra                indUnk  female   \n",
       "11     1       Scorpio                indUnk  female   \n",
       "12     1         Virgo            RealEstate    male   \n",
       "13     0        Pisces                indUnk  female   \n",
       "14     0         Virgo            Technology    male   \n",
       "15     0   Sagittarius            Publishing    male   \n",
       "16     0         Libra            Publishing    male   \n",
       "17     0           Leo  Communications-Media  female   \n",
       "18     0         Libra            RealEstate  female   \n",
       "19     0      Aquarius                indUnk  female   \n",
       "20     1        Pisces               Student    male   \n",
       "21     0       Scorpio                indUnk  female   \n",
       "22     0        Cancer                indUnk    male   \n",
       "23     1        Cancer               Student  female   \n",
       "24     1         Libra                indUnk  female   \n",
       "25     1        Gemini               Student  female   \n",
       "26     0         Aries             Education  female   \n",
       "27     1       Scorpio               Student  female   \n",
       "28     1         Libra                indUnk  female   \n",
       "29     0         Aries             Education    male   \n",
       "..   ...           ...                   ...     ...   \n",
       "936    0        Gemini                indUnk    male   \n",
       "937    0       Scorpio            Technology    male   \n",
       "938    0        Taurus           Engineering  female   \n",
       "939    0         Virgo               Fashion  female   \n",
       "940    0        Cancer            Technology    male   \n",
       "941    0       Scorpio                   Law  female   \n",
       "942    0     Capricorn                indUnk  female   \n",
       "943    1       Scorpio               Student  female   \n",
       "944    0        Taurus                indUnk  female   \n",
       "945    1        Taurus               Student  female   \n",
       "946    0         Aries              Internet    male   \n",
       "947    1         Virgo               Student    male   \n",
       "948    1      Aquarius        HumanResources    male   \n",
       "949    0   Sagittarius           Engineering    male   \n",
       "950    0   Sagittarius                indUnk  female   \n",
       "951    1      Aquarius                indUnk  female   \n",
       "952    0      Aquarius                  Arts    male   \n",
       "953    0        Taurus           Agriculture  female   \n",
       "954    1        Cancer               Student    male   \n",
       "955    1        Pisces               Student    male   \n",
       "956    1   Sagittarius               Student    male   \n",
       "957    1     Capricorn            Accounting    male   \n",
       "958    0        Pisces            Consulting  female   \n",
       "959    1        Taurus               Student    male   \n",
       "960    1        Cancer                indUnk  female   \n",
       "961    0        Gemini            Non-Profit  female   \n",
       "962    0   Sagittarius                indUnk    male   \n",
       "963    0        Gemini                indUnk    male   \n",
       "964    0         Libra                indUnk  female   \n",
       "965    1           Leo               Student    male   \n",
       "\n",
       "                                                  post  user_id  \n",
       "0    Well, everyone got up and going this morning. ...  1000331  \n",
       "1    Yeah, sorry for not writing for a whole there,...  1000866  \n",
       "2    cupid,please hear my cry, cupid, please let yo...  1004904  \n",
       "3    and did i mention that i no longer have to dea...  1005076  \n",
       "4    B-Logs: The Business Blogs Paradox    urlLink ...  1005545  \n",
       "5    1/03 DrKioni.com Awarded ByRegion.net Healers ...  1007188  \n",
       "6    Friday    My dear wife was walking on her grad...  /100812  \n",
       "7    Sorry, but I gotta..I couldn't remember the wo...  1008329  \n",
       "8    Planning the Marathon   I checked Active.com, ...  1009572  \n",
       "9    The astute among you will note that this run i...  1011153  \n",
       "10   MSN conversation: 11.17am   Iggbalbollywall  (...  1011289  \n",
       "11   Hey!!! Tonight kids, I saw Harry Potter and th...  1011311  \n",
       "12   Hey you clowns and kids with Down's. Sorry I h...  1013637  \n",
       "13   aint no such things as halfway crooks.   this ...  1015252  \n",
       "14   well, the retreat was a success, i think. at l...  1015556  \n",
       "15   In his  urlLink February 14, 2003 post  Ray Ma...  1016560  \n",
       "16   as astute followers of the  urlLink Assistant ...  1016738  \n",
       "17   You love me... I have you here by my side... O...  1016787  \n",
       "18   Dear Susan,     You are a fat Wombat... I hate...  1019224  \n",
       "19   yay! it changed! :) i think i get it now. you ...  1019622  \n",
       "20   Yes i survived not eating for 24 hours, I am g...  1019710  \n",
       "21   IN THE SPACESHIP, THE SILVER SPACESHIP�  I fou...  1021779  \n",
       "22   Aaaaaahhhh....... mornings! know that it is st...  1022037  \n",
       "23   All right, I officially live in THE nut house....  1022086  \n",
       "24   wait...now dat i think about it...dat GBC thin...  1024234  \n",
       "25   Lily's Lullaby- Part SIX  It was cold that eve...  1025783  \n",
       "26   I've posted a new test on my AIM profile...you...  1026164  \n",
       "27   Hello hello hello again. It's lovely to speak ...  1026443  \n",
       "28   OK here goes nothing, I'm starting over. My ol...  1028027  \n",
       "29   Wednesday of this week, my wife Elizabeth and ...  1028257  \n",
       "..                                                 ...      ...  \n",
       "936  Slept at 10 last night. That's something I hav...  1757463  \n",
       "937  week of august 24 - 30, 2003    urlLink Death ...  1758069  \n",
       "938  today i feel so cute ! don´t know why, maybe b...  1758432  \n",
       "939  hi. i'm back. another week past. so, all the e...  1759323  \n",
       "940  I started my new workout regime today.  This r...  1759489  \n",
       "941  I'm 40 years old, female, single, working as a...  1760992  \n",
       "942  Days off always make me want to waste time wit...  1761003  \n",
       "943  This is how i exactly feel...  Scorpio October...  1761133  \n",
       "944  It works.. I love doing this. :DI found a nift...  1761174  \n",
       "945  hiz... juz wasting time and testing some stuff...  1762421  \n",
       "946  This blog is about www.monto-dash.com and its ...  1763114  \n",
       "947  [Listening to: Miss You -  urlLink Aaliyah  - ...  1764527  \n",
       "948  i wouldn't believe too much about what you hea...  1764573  \n",
       "949  urlLink movabletype.org  produced a great tool...  1764916  \n",
       "950  so, apparently, the mountain decided to come t...  1767662  \n",
       "951  Hey yaw! i kno i havent written in a while so ...  1769548  \n",
       "952  So, I had two topics I was considering for tod...  1769576  \n",
       "953  urlLink f**kin' boogers.GAMEDAY!!!   Just got ...  1770781  \n",
       "954  Wow, my first post in a long line of posts to ...  1770846  \n",
       "955  \"going under\"  The rise of exams stress certai...  1771307  \n",
       "956  hey ppl..im in victoria now, its so damn hot!!...  1772041  \n",
       "957  please do feel free to leave comments if you w...  1772134  \n",
       "958  i dun like to wait. i dun like to sit n sulk. ...  1772503  \n",
       "959  hey guys, its been a long time. I dont hav a r...  1774151  \n",
       "960  steven sent me this thing so i decided to fill...  1774842  \n",
       "961  so i move to california a month ago. i am incr...  1776600  \n",
       "962  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbs...  1777155  \n",
       "963  without the jukebox. The jukebox that Elaine s...  1778579  \n",
       "964  We got back from the conference to bunch of in...  1778822  \n",
       "965  Aint I lousy with mine updates.  I'm pretty ex...  1779446  \n",
       "\n",
       "[966 rows x 6 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "data = load_data(DATA_FILE_PATH_PATTERN, ratio=0.05)\n",
    "# Even with 16GB RAM, it was not enough to hold whole data,\n",
    "# so please use small data, but more than 10% of whole data.\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem11.\n",
    "\n",
    "Now, we will split the data into three sets,\n",
    "\n",
    "train, cv, and test sets.\n",
    "\n",
    "Our goal is to predict whether the blogger is teenager or not.\n",
    "\n",
    "Please make split_dataset function.\n",
    "\n",
    "which returns (feature_train, feature_cv, feature_test, answer_train, answer_cv, answer_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Train, CV, Test\n",
    "def split_dataset(data, ratio=(.6, .2, .2), shuffle=False):\n",
    "    if sum(ratio) != 1.:\n",
    "        print \"Invalid data split ratio\", ratio\n",
    "        return\n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    Add the condition whether shuffle the data or not.\n",
    "    \n",
    "    And then, split the data into train, validation, test set\n",
    "    \n",
    "    with given ratio.\n",
    "    \n",
    "    y values are from age column,\n",
    "    \n",
    "    and x values are from post column.\n",
    "    \n",
    "    user_id, gender, age, fields, constellation\n",
    "    \"\"\"\n",
    "    x_train=list()\n",
    "    y_train=list()\n",
    "    x_cv=list()\n",
    "    y_cv=list()\n",
    "    x_test=list()\n",
    "    y_test=list()\n",
    "    m = len(data)\n",
    "    randli=[]\n",
    "    for i in range(0,m):\n",
    "        randli.append(i)\n",
    "    if(shuffle):\n",
    "        randli.shuffle()\n",
    "    for i in range(0,m*6/10):\n",
    "        x_train.append(data['post'][randli[i]])\n",
    "        y_train.append(data['age'][randli[i]])\n",
    "    for i in range(m*6/10, m*8/10):\n",
    "        x_cv.append(data['post'][randli[i]])\n",
    "        y_cv.append(data['age'][randli[i]])\n",
    "    for i in range(m*8/10,m):\n",
    "        x_test.append(data['post'][randli[i]])\n",
    "        y_test.append(data['age'][randli[i]])\n",
    "    ####################################################\n",
    "    \"\"\"\n",
    "    x_train=pd.DataFrame(x_train)\n",
    "    y_train=pd.DataFrame(y_train)\n",
    "    x_cv=pd.DataFrame(x_cv)\n",
    "    y_cv=pd.DataFrame(y_cv)\n",
    "    x_test=pd.DataFrame(x_test)\n",
    "    y_test=pd.DataFrame(y_test)\n",
    "    \"\"\"\n",
    "    return (x_train, x_cv, x_test, y_train, y_cv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  579\n",
      "CV set    :  193\n",
      "Test set :  194\n"
     ]
    }
   ],
   "source": [
    "(x_train, x_cv, x_test, y_train, y_cv, y_test) = split_dataset(df)\n",
    "print \"Train set: \" , len(x_train)\n",
    "print \"CV set    : \", len(x_cv)\n",
    "print \"Test set : \", len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem12.\n",
    "\n",
    "From the previous assignment,\n",
    "\n",
    "we learned what BOW(bag of words) is.\n",
    "\n",
    "Now, we will make BOW from the training set's post data.\n",
    "\n",
    "Please fill the marked blank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bow_matrix(data, word_index=None, tf_mode=False):\n",
    "    # For Training set (with creating Word index)\n",
    "    if word_index == None:\n",
    "        return __get_bow_matrix_trains(data, tf_mode=tf_mode)\n",
    "    # for CV or Test set\n",
    "    else:\n",
    "        return __get_bow_matrix(data, word_index=word_index, tf_mode=tf_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_bow_matrix_trains(train_set, tf_mode):\n",
    "    bows = list()\n",
    "    \n",
    "    bows = [\n",
    "        __get_bow_dict(d, tf_mode=tf_mode)\n",
    "        for d in train_set\n",
    "    ]\n",
    "                \n",
    "    # Create word length dictionary\n",
    "    word_len = dict()\n",
    "    for bow in bows:\n",
    "        #bow of each post\n",
    "        for word in bow.keys():\n",
    "            if word not in word_len:\n",
    "                word_len[word] = 1\n",
    "            else:\n",
    "                word_len[word] += 1\n",
    "                \n",
    "    final_word_list = __get_word_list_by_cf(word_len,5000)\n",
    "    \n",
    "    # bow dict -> matrix\n",
    "    if tf_mode:\n",
    "        bow_matrix = np.zeros((len(bows), len(final_word_list)), dtype=np.uint16)\n",
    "    else:\n",
    "        bow_matrix = np.zeros((len(bows), len(final_word_list)), dtype=np.bool)\n",
    "    \n",
    "    for bow_idx in range(len(bows)):\n",
    "        bow = bows[bow_idx]\n",
    "        for w in bow:\n",
    "            if w in final_word_list:\n",
    "                bow_matrix[bow_idx, final_word_list.index(w)] = bow[w]\n",
    "            \n",
    "    return bow_matrix, final_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Training set.\n",
    "def __get_bow_dict(text, word_index=None, tf_mode = False):\n",
    "    bow = dict()\n",
    "\n",
    "    for word in wordpunct_tokenize(text):\n",
    "        # if word is in stopword_set\n",
    "        # only accept fully alphabet\n",
    "        word = word.lower()\n",
    "        if word in stopword_set or not word.isalpha():\n",
    "            continue      \n",
    "            \n",
    "        stem = stemmer.stem(word)\n",
    "        if word_index is not None:\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "                \n",
    "        \"\"\"\n",
    "        Create BOW dict\n",
    "        \"\"\"\n",
    "        # boolean mode (true or false)\n",
    "        if not tf_mode:\n",
    "            bow[stem] = 1\n",
    "        # Term Frequency Mode (bow[stem] = stemmed word occurance)\n",
    "        else:\n",
    "            if stem not in bow:\n",
    "                bow[stem] = 0\n",
    "            bow[stem] = bow[stem] + 1\n",
    "\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "# because the word list is too big, reduce word list, by collection frequency.\n",
    "def __get_word_list_by_cf(word_len_dict, k = 1000):\n",
    "    \n",
    "    li = []\n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    From the word_len_dict,\n",
    "    \n",
    "    please make frequency top k word list\n",
    "    \n",
    "    and return the list\n",
    "    \n",
    "    word_len_dict['word'] -> 그 word가 나타난 빈도수.\n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    \n",
    "    top_k_word_list=li\n",
    "    sorted_word = sorted(word_len_dict.items(), key=operator.itemgetter(1))\n",
    "    i=0\n",
    "    for item in sorted_word:\n",
    "        if(i<k):\n",
    "            li.append(item)\n",
    "        else:\n",
    "            break\n",
    "        i=i+1\n",
    "    return top_k_word_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_bow_matrix(dataset, word_index, tf_mode):\n",
    "    \n",
    "    if tf_mode:\n",
    "        matrix = np.zeros((len(dataset), len(word_index)), dtype=np.uint16)\n",
    "    else:\n",
    "        matrix = np.zeros((len(dataset), len(word_index)), dtype=np.bool)\n",
    "    \n",
    "    d_idx = 0\n",
    "    for data in dataset:\n",
    "        bow = __get_bow_dict(data, word_index, tf_mode)\n",
    "        for w in bow.keys():\n",
    "            if w in word_index:\n",
    "                matrix[d_idx, word_index.index(w)] = bow[w]\n",
    "        d_idx += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  527.241605997\n",
      "Elapsed Time:  505.777935028\n",
      "Elapsed Time:  436.170535088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_t = time.time()\n",
    "bow_matrix_train, word_index = get_bow_matrix(x_train, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)\n",
    "start_t = time.time()\n",
    "bow_matrix_cv = get_bow_matrix(x_cv, word_index, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)\n",
    "start_t = time.time()\n",
    "bow_matrix_test = get_bow_matrix(x_test, word_index, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem13.\n",
    "\n",
    "Now, it's time to fit the model,\n",
    "\n",
    "and get the result.\n",
    "\n",
    "Please use your validation set to fit the model,\n",
    "\n",
    "and then predict the value of test set.\n",
    "\n",
    "Use sklearn.metrics.precision_recall_fscore_support method ([description](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)),\n",
    "\n",
    "and use fscore as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary :  (0.47488523181830383, 0.68911917098445596, 0.56228742172351309, None)\n",
      "macro :  (0.34455958549222798, 0.5, 0.40797546012269936, None)\n",
      "micro :  (0.68911917098445596, 0.68911917098445596, 0.68911917098445596, None)\n",
      "weighted :  (0.47488523181830383, 0.68911917098445596, 0.56228742172351309, None)\n",
      "max is micro.\n",
      "test :  (0.65463917525773196, 0.65463917525773196, 0.65463917525773196, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahroobe/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:12: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n"
     ]
    }
   ],
   "source": [
    "svc_model = SVC()\n",
    "\n",
    "###########################################################\n",
    "########## fit the model by svc model              ########\n",
    "########## and predict the value of validation set ########\n",
    "########## set the parameter,                      ########\n",
    "########## then predict the test set               ########\n",
    "###########################################################\n",
    "svc_model.fit(bow_matrix_train, y_train)\n",
    "pred_cv = svc_model.predict(bow_matrix_cv)\n",
    "\n",
    "prf = sklearn.metrics.precision_recall_fscore_support(y_cv, pred_cv,average='binary',pos_label=None)\n",
    "print \"binary : \" , prf\n",
    "prf = sklearn.metrics.precision_recall_fscore_support(y_cv, pred_cv,average='macro',pos_label=None)\n",
    "print \"macro : \" , prf\n",
    "prf = sklearn.metrics.precision_recall_fscore_support(y_cv, pred_cv,average='micro',pos_label=None)\n",
    "print \"micro : \", prf\n",
    "prf = sklearn.metrics.precision_recall_fscore_support(y_cv, pred_cv,average='weighted',pos_label=None)\n",
    "print \"weighted : \", prf\n",
    "print \"max is micro.\"\n",
    "\n",
    "pred_test = svc_model.predict(bow_matrix_test)\n",
    "prf = sklearn.metrics.precision_recall_fscore_support(y_test, pred_test,average='micro',pos_label=None)\n",
    "print  \"test : \" , prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79127726  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# print the prediction result of test set\n",
    "pred_test = svc_model.predict(bow_matrix_test)\n",
    "prf_test = sklearn.metrics.precision_recall_fscore_support(y_test, pred_test)\n",
    "print prf_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.65463917525773196, 0.65463917525773196, 0.65463917525773196, None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
